# Capture The Flag (CTF) - Autonomous Navigation Challenge

## Overview

This project simulates a **Capture The Flag** challenge from a self-driving vehicle perspective. The robot must autonomously navigate to find and reach a target object (the "flag") using computer vision and AI decision-making, similar to how autonomous vehicles navigate to destinations.

**Goal:** Navigate to and "capture" a target object (e.g., chair, ball, person) using only camera vision and AI/rule-based decision making.

---

## System Architecture

The navigation system consists of **3 main components** that work together to achieve autonomous navigation:

### 1. **Navigation** (`navigate_with_yolo()`)
**Purpose:** Main navigation system toward detected target objects

**Two Operating Modes:**

#### Rule-Based Navigation (Default)
Hardcoded logic based on object detection:
- âœ… Target centered â†’ move forward
- âœ… Target left â†’ turn left  
- âœ… Target right â†’ turn right
- âœ… Target too close (large area) â†’ stop (FLAG CAPTURED!)
- âœ… Blocker detected â†’ avoid right

#### AI-Based Navigation (`ai_decide=True`)
Uses Ollama LLM for intelligent decision-making:
- ğŸ§  Sends annotated image to AI with context
- ğŸ§  AI analyzes: target position, obstacles, history
- ğŸ§  AI decides: forward/left/right/stop
- ğŸ§  Uses decision history for context-aware navigation
- ğŸ§  Function: `ai_navigation_decision()`

**Location:** `utils/navigation_utils.py` lines 645-855

---

### 2. **Smart Search** (Target Not Visible)
**Purpose:** Scan environment to locate the target when it's out of view

**Two Implementations:**

#### Active: `smart_search_for_target()` (Rule-Based Camera Scanning)
When target disappears from view:
1. ğŸ“· Rotate camera left â†’ capture â†’ detect objects
2. ğŸ“· Rotate camera right â†’ capture â†’ detect objects  
3. ğŸ¯ Compare detections on both sides
4. ğŸ”„ Robot turns toward side where target was found
5. ğŸ” Return camera to center, continue navigation

**Location:** `utils/navigation_utils.py` lines 485-642

#### Disabled: `ai_search_decision()` (AI-Based Search - DEAD CODE)
Alternative AI-powered search approach:
- ğŸ“¸ Captures 4 camera angles (left, center-left, center-right, right)
- ğŸ–¼ï¸ Combines images into 2x2 grid
- ğŸ§  Asks Ollama AI which direction to search
- âš ï¸ Currently disabled in `fcam.py` (line 669: `if ai_decide or False:`)

**Location:** `utils/navigation_utils.py` lines 166-350

---

### 3. **Vision-Based Stuck Recovery** 
**Purpose:** Detect and escape when robot gets stuck (obstacle collision, wall, etc.)

**How It Works:**
1. ğŸ” **Stuck Detection:** Compares consecutive frames
   - If frames are >97% similar for 3+ frames â†’ robot is stuck
   
2. ğŸ“· **Environment Scanning:** Captures 3 views
   - Left angle
   - Center angle  
   - Right angle

3. ğŸ¯ **Escape Direction Selection:** Chooses direction with most open space
   - Analyzes object count and sizes in each view
   - Selects direction with fewest/smallest obstacles

4. ğŸš— **Evasive Maneuver:** Executes escape sequence
   - Move backward (clear immediate obstacle)
   - Turn toward open direction
   - Resume normal navigation

**Location:** `utils/navigation_utils.py` lines 352-483

---

## Call Hierarchy

```
run_navigation_loop() [fcam.py]
â”‚
â”œâ”€â†’ is_robot_stuck()                                   # Frame comparison
â”‚   â””â”€â†’ vision_based_stuck_recovery()                  # If stuck detected
â”‚       â”œâ”€â†’ Capture 3 angles
â”‚       â”œâ”€â†’ Analyze each direction
â”‚       â””â”€â†’ Execute backward + turn
â”‚
â”œâ”€â†’ detect_objects_yolo()                              # YOLO detection
â”‚   â””â”€â†’ Returns: objects list + annotated image
â”‚
â”œâ”€â†’ navigate_with_yolo()                               # Main navigation
â”‚   â”‚
â”‚   â”œâ”€â†’ AI Mode (ai_decide=True):
â”‚   â”‚   â””â”€â†’ ai_navigation_decision()                   # Ollama decision
â”‚   â”‚       â”œâ”€â†’ Send image + context to AI
â”‚   â”‚       â”œâ”€â†’ Parse AI response
â”‚   â”‚       â””â”€â†’ Return: forward/left/right/stop
â”‚   â”‚
â”‚   â””â”€â†’ Rule-Based Mode (ai_decide=False):
â”‚       â””â”€â†’ Hardcoded logic (position-based turns)
â”‚
â””â”€â†’ smart_search_for_target()                          # If target not found
    â”‚
    â”œâ”€â†’ Active: Rule-based camera scan
    â”‚   â”œâ”€â†’ Rotate camera left + detect
    â”‚   â”œâ”€â†’ Rotate camera right + detect
    â”‚   â””â”€â†’ Turn robot toward target side
    â”‚
    â””â”€â†’ DEAD CODE: ai_search_decision()                # Disabled
        â”œâ”€â†’ Capture 4-angle grid
        â””â”€â†’ Ask AI for search direction
```

---

## Capture The Flag Workflow

### Phase 1: Target Acquisition
1. ğŸ“· **Vision:** Capture camera frame
2. ğŸ¯ **Detection:** YOLO identifies objects in view
3. ğŸ” **Search:** If target not found â†’ `smart_search_for_target()`

### Phase 2: Navigation
1. ğŸ§  **Decision:** AI or rule-based decides movement
2. ğŸš— **Execution:** Send command to robot (forward/left/right)
3. ğŸš§ **Obstacle Avoidance:** Avoid blockers while navigating

### Phase 3: Stuck Recovery
1. ğŸ” **Detection:** Monitor frame similarity
2. ğŸš¨ **Trigger:** 3+ identical frames â†’ stuck
3. ğŸ”„ **Recovery:** `vision_based_stuck_recovery()` escapes
4. â–¶ï¸ **Resume:** Return to navigation

### Phase 4: Flag Capture
1. ğŸ¯ **Approach:** Target centered, moving forward
2. ğŸ“ **Distance Check:** Target area > 30% of frame
3. ğŸ **SUCCESS:** Stop robot â†’ FLAG CAPTURED!

---

## Configuration

Edit `fcam.py` to configure CTF parameters:

```python
if __name__ == '__main__':
    use_ollama = False       # Deprecated full Ollama mode
    ai_decide = True         # Enable AI decision-making
    target = 'chair'         # The "flag" to capture
    use_segmentation = True  # Better object detection
    capture_video = True     # Record navigation session
    
    main(use_ollama, ai_decide, target, use_segmentation, capture_video)
```

### Supported "Flags" (COCO Classes)
- `chair`, `person`, `cup`, `bottle`, `ball`, `car`, `dog`, `cat`, etc.

---

## Self-Driving Vehicle Parallels

This CTF challenge mirrors real autonomous vehicle problems:

| Robot Challenge | Autonomous Vehicle Equivalent |
|----------------|------------------------------|
| Find target object | Navigate to GPS destination |
| Obstacle avoidance | Pedestrian/vehicle detection |
| Stuck recovery | Handle deadlock situations |
| Smart search | Re-routing when lost |
| AI decision-making | Path planning algorithms |
| Vision-based navigation | Camera-based SLAM |

---

## Video Recording

All navigation sessions are recorded with embedded logs showing:
- ğŸ¯ Object detections
- ğŸ§  AI decisions (if enabled)
- ğŸš— Movement commands
- ğŸ” Search actions
- ğŸš¨ Stuck recovery events

Videos saved to: `videos/robot_video_YYYYMMDD_HHMMSS.mp4`

---

## Technical Stack

- **Vision:** YOLO v8/v11 (detection + segmentation)
- **AI:** Ollama + Gemma3:4b (local LLM)
- **Control:** Socket-based robot communication
- **Recording:** OpenCV video writer with split-screen layout
- **Visualization:** Matplotlib real-time path tracking

---

## Success Metrics

- âœ… **Flag Captured:** Robot reaches target and stops
- ğŸ“ **Distance:** Minimize path length to target
- â±ï¸ **Time:** Minimize time to capture
- ğŸš§ **Collisions:** Zero collisions (stuck events)
- ğŸ§  **AI Decisions:** Quality of AI reasoning (if enabled)
